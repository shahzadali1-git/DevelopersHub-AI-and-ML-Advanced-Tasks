{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "019ebc09-e3a6-46ca-9284-20f8ddfa4e2e",
   "metadata": {},
   "source": [
    "# Import Important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125451ce-e3a8-434a-b628-bc84ecd0e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4493edac-6ee2-42d3-b634-b5b933d5c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570bd099-8fe4-4634-ba01-078bdae25e09",
   "metadata": {},
   "source": [
    "# PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab89b7c-5c02-40ad-8975-731b5df9e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.object = object # Fix for the FutureWarning\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dataset_path = \"Houses-dataset/HousesDataset/\"\n",
    "info_file = os.path.join(dataset_path, \"HousesInfo.txt\")\n",
    "\n",
    "print(f\" Target file: {info_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ed574-9040-4089-9dec-0d9ec09a9ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Search for the file automatically\n",
    "def find_houses_info():\n",
    "    for root, dirs, files in os.walk(\".\"):\n",
    "        if \"HousesInfo.txt\" in files:\n",
    "            return os.path.join(root, \"HousesInfo.txt\")\n",
    "    return None\n",
    "\n",
    "info_file = find_houses_info()\n",
    "\n",
    "if info_file:\n",
    "    print(f\"‚úÖ Found data at: {info_file}\")\n",
    "    # 2. Extract the folder path so we can find images later\n",
    "    dataset_path = os.path.dirname(info_file)\n",
    "    \n",
    "    # 3. Load the data\n",
    "    cols = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n",
    "    df = pd.read_csv(info_file, sep=\" \", header=None, names=cols)\n",
    "    print(\"‚úÖ Data Loaded Successfully!\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"‚ùå Error: HousesInfo.txt is still missing.\")\n",
    "    print(\"Verify that your folder isn't empty (sometimes 'No space left' results in empty folders).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602fbd63-24f1-4a9a-bf78-1b169f97324a",
   "metadata": {},
   "source": [
    "# 1. LOAD TABULAR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc8a60-f6b6-4457-a444-a25be145689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Using the directory we just discovered\n",
    "dataset_path = os.path.dirname(info_file) \n",
    "\n",
    "def load_and_show_samples(df, path, count=5):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(count):\n",
    "        # The dataset typically uses 1-based indexing for filenames\n",
    "        img_name = f\"{i+1}_main.jpg\" \n",
    "        img_path = os.path.join(path, img_name)\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            plt.subplot(1, count, i + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Price: ${df.iloc[i]['price']:,}\")\n",
    "            plt.axis(\"off\")\n",
    "        else:\n",
    "            print(f\"Skipping {img_name}: Not found in {path}\")\n",
    "    plt.show()\n",
    "\n",
    "load_and_show_samples(df, dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b59c7f-9223-4ecd-b529-82101c5a7643",
   "metadata": {},
   "source": [
    "# Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d79f3a-1761-41e9-8bcb-e8a89349bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Scale Tabular Features\n",
    "# We use Bedrooms, Bathrooms, and Area as our features\n",
    "scaler = MinMaxScaler()\n",
    "tabular_features = scaler.fit_transform(df[[\"bedrooms\", \"bathrooms\", \"area\"]])\n",
    "\n",
    "# 2. Scale the Target (Price)\n",
    "# We divide by the maximum price so the target is between 0 and 1\n",
    "max_price = df[\"price\"].max()\n",
    "target_prices = df[\"price\"].values / max_price\n",
    "\n",
    "print(f\"‚úÖ Tabular data normalized. Max price for scaling: ${max_price:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0093905-a66d-44fb-b7e2-799a448f85b5",
   "metadata": {},
   "source": [
    "# 2. LOAD IMAGES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3883a137-d13c-4943-931f-8fad2d5a0173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Look at the first 10 files in your dataset folder\n",
    "files = os.listdir(dataset_path)\n",
    "print(\"Actual files found in folder:\")\n",
    "print(files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6bfcff-ebcd-40ba-8535-d5a18f82c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_all_images_robust(df, path):\n",
    "    all_images = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    print(\"Scanning for images...\")\n",
    "    for i in df.index:\n",
    "        # We try the most common filename formats\n",
    "        possible_names = [f\"{i+1}_main.jpg\", f\"{i+1}_frontal.jpg\", f\"{i+1}.jpg\"]\n",
    "        found = False\n",
    "        \n",
    "        for name in possible_names:\n",
    "            img_path = os.path.join(path, name)\n",
    "            if os.path.exists(img_path):\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is not None:\n",
    "                    image = cv2.resize(image, (64, 64))\n",
    "                    all_images.append(image / 255.0)\n",
    "                    valid_indices.append(i)\n",
    "                    found = True\n",
    "                    break\n",
    "        \n",
    "        if not found and i < 5: # Only print first few errors to save space\n",
    "            print(f\"Still can't find image for row {i+1}\")\n",
    "\n",
    "    return np.array(all_images), valid_indices\n",
    "\n",
    "# Execute the robust loader\n",
    "images_data, valid_indices = load_all_images_robust(df, dataset_path)\n",
    "\n",
    "# IMPORTANT: We must filter our tabular data to match only the images we actually found\n",
    "df_filtered = df.iloc[valid_indices]\n",
    "tabular_features_filtered = tabular_features[valid_indices]\n",
    "target_prices_filtered = target_prices[valid_indices]\n",
    "\n",
    "print(f\"‚úÖ Successfully matched {len(images_data)} images with tabular data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f15657f-d066-4514-ab1d-9bb75576fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, concatenate\n",
    "\n",
    "# BRANCH 1: MLP for Tabular Data\n",
    "tab_input = Input(shape=(3,)) # bedrooms, bathrooms, area\n",
    "x = Dense(16, activation=\"relu\")(tab_input)\n",
    "x = Dense(8, activation=\"relu\")(x)\n",
    "\n",
    "# BRANCH 2: CNN for Image Data\n",
    "img_input = Input(shape=(64, 64, 3))\n",
    "y = Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\")(img_input)\n",
    "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
    "y = Flatten()(y)\n",
    "y = Dense(16, activation=\"relu\")(y)\n",
    "\n",
    "# MERGE BRANCHES\n",
    "combined = concatenate([x, y])\n",
    "\n",
    "# FINAL REGRESSION HEAD\n",
    "z = Dense(4, activation=\"relu\")(combined)\n",
    "z = Dense(1, activation=\"linear\")(z) # Linear for price prediction\n",
    "\n",
    "model = Model(inputs=[tab_input, img_input], outputs=z)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "print(\"‚úÖ Multimodal Model Architecture Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756ef8c-cd37-4b9c-90cd-a3472492d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def find_image_folder():\n",
    "    for root, dirs, files in os.walk(\".\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\") and \"_\" in file:\n",
    "                print(f\"‚úÖ Found images in: {root}\")\n",
    "                return root\n",
    "    return None\n",
    "\n",
    "image_dir = find_image_folder()\n",
    "\n",
    "if not image_dir:\n",
    "    print(\"‚ùå ERROR: No .jpg files found. Please check if your dataset download finished.\")\n",
    "else:\n",
    "    def load_images_any_name(df, path):\n",
    "        all_images = []\n",
    "        valid_indices = []\n",
    "        # Get list of all files in that folder once\n",
    "        files_in_folder = os.listdir(path)\n",
    "        \n",
    "        for i in df.index:\n",
    "            # Look for ANY file that starts with the house number (e.g., \"1_\")\n",
    "            matching_files = [f for f in files_in_folder if f.startswith(f\"{i+1}_\") and f.endswith(\".jpg\")]\n",
    "            \n",
    "            if matching_files:\n",
    "                img_full_path = os.path.join(path, matching_files[0])\n",
    "                image = cv2.imread(img_full_path)\n",
    "                if image is not None:\n",
    "                    image = cv2.resize(image, (64, 64))\n",
    "                    all_images.append(image / 255.0)\n",
    "                    valid_indices.append(i)\n",
    "        \n",
    "        return np.array(all_images), valid_indices\n",
    "\n",
    "    images_data, valid_indices = load_images_any_name(df, image_dir)\n",
    "    tabular_features_filtered = tabular_features[valid_indices]\n",
    "    target_prices_filtered = target_prices[valid_indices]\n",
    "    print(f\"Final Count - Images: {len(images_data)}, Tabular: {len(tabular_features_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3519cde3-c817-4b73-9d32-1e0cd5b875b7",
   "metadata": {},
   "source": [
    "# Split and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4996b3-b3b1-456d-92f4-fbb4c816549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, concatenate\n",
    "\n",
    "# 1. SPLIT: 75% for training, 25% for testing\n",
    "(trainTab, testTab, trainImg, testImg, trainY, testY) = train_test_split(\n",
    "    tabular_features_filtered, images_data, target_prices_filtered, \n",
    "    test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# 2. DEFINE MULTIMODAL ARCHITECTURE\n",
    "# Tabular Branch\n",
    "tab_in = Input(shape=(3,))\n",
    "x = Dense(16, activation=\"relu\")(tab_in)\n",
    "x = Dense(8, activation=\"relu\")(x)\n",
    "\n",
    "# Image Branch (CNN)\n",
    "img_in = Input(shape=(64, 64, 3))\n",
    "y = Conv2D(16, (3, 3), activation='relu', padding=\"same\")(img_in)\n",
    "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
    "y = Flatten()(y)\n",
    "y = Dense(16, activation='relu')(y)\n",
    "\n",
    "# Merge the two \"thoughts\" together\n",
    "combined = concatenate([x, y])\n",
    "\n",
    "# Final prediction layers\n",
    "z = Dense(4, activation=\"relu\")(combined)\n",
    "z = Dense(1, activation=\"linear\")(z)\n",
    "\n",
    "model = Model(inputs=[tab_in, img_in], outputs=z)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "# 3. START TRAINING\n",
    "print(\"Model training in progress...\")\n",
    "model.fit(x=[trainTab, trainImg], y=trainY, \n",
    "          validation_data=([testTab, testImg], testY), \n",
    "          epochs=20, batch_size=8)\n",
    "\n",
    "# 4. CALCULATE REAL-WORLD ERROR\n",
    "preds = model.predict([testTab, testImg])\n",
    "# Convert the 0-1 scaling back to actual dollars\n",
    "mae_dollars = np.mean(np.abs(preds.flatten() - testY)) * max_price\n",
    "\n",
    "print(f\"\\n\" + \"=\"*30)\n",
    "print(f\"üöÄ TASK COMPLETE!\")\n",
    "print(f\"Final Prediction Error: ${mae_dollars:,.2f}\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80cff22-913d-4d3e-96b6-a4f75b9b39b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the \"history =\" part is there before model.fit\n",
    "history = model.fit(x=[trainTab, trainImg], y=trainY, \n",
    "                  validation_data=([testTab, testImg], testY), \n",
    "                  epochs=20, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531eb45-9673-432a-9dd1-7fb3c1cf1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Training vs Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Learning Curve (Loss)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac8a75-d05e-4406-8160-632fa20d99c1",
   "metadata": {},
   "source": [
    "# Final Results for GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f12e5f8-3bcc-4d81-878c-cc43246aa71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, mae = model.evaluate([testTab, testImg], testY)\n",
    "print(f\"Final MAE (Mean Absolute Error): ${mae:,.2f}\")\n",
    "print(f\"Final RMSE: ${np.sqrt(mse):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad353d47-77cc-476f-982a-343b017ed0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01492a44-bcd0-4e48-bf07-737b7bbe63d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
